<!DOCTYPE html>
<html>
  <head>
    <title>Exploring Llama 3.2 with Rust and Hugging Face Tools</title>
    <link rel="stylesheet" href="../../cv/sakura.css" />
    <link rel="stylesheet" href="../../common/prism.css" />
  </head>
  <body>
    <script src="../../common/prism.js"></script>

    <div style="text-align: center">
      <h2>Exploring Llama 3.2 with Rust and Hugging Face Tools</h2>
      Rei Sato<br />Dec. 2024
    </div>

    <h2>1. Introduction</h2>

    <h2>2. Llama 3.2</h2>
    Llama is one of the most widely used generative language models, developed and released by Meta. This section provides an overview of how Llama operates, though many
    aspects are common to other large language models (LLMs) as well. Llama adopts a decoder-only Transformer architecture, where it takes a token sequence converted from
    an input string and predicts the subsequent tokens. It can also predict multiple tokens consecutively. In such cases, previously generated tokens are appended to the
    end of the input token sequence in an auto-regressive manner. A token is an ID corresponding to a string, which is converted into dense features in the initial
    embedding layer of the Transformer.

    <h3>2.1. Tokenizer</h3>
    Llama 3.2 utilizes Hugging Face Tokenizers, which is implemented in Rust. Let's take a look at the chain of references from an abstracted Python class to the Rust
    implementation.
    <br />
    The pre-trained Llama 3.2 model is available on the Hugging Face Hub and includes tokenizer-related files. In particular, tokenizer_config.json is a file that stores
    the tokenizer settings used by the Hugging Face Transformers library, and
    <code>PreTrainedTokenizerFast</code> is specified as the tokenizer for Llama 3.2 (<a
      href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/blob/0907763fe3a4b2e195d576cbc0144e38cdabdce5/tokenizer_config.json#L2061"
      >ref.</a
    >). When transformers.PreTrainedTokenizerFast is instantiated, a TokenizerFast class is created, but in reality, its underlying implementation is tokenizers.Tokenizer
    class. The Tokenizer class in the tokenizer library for Python is implemented in Rust and published as a binding for Python (<a
      href="https://github.com/huggingface/tokenizers/blob/v0.20.3/bindings/python/src/tokenizer.rs#L465"
      >ref.</a
    >).

    <h3>2.2. Safetensors</h3>

    <h2>3. Llama in Rust</h2>
    To gain a deeper understanding of Llama, I implemented its inference code in Rust. This implementation has undergone limited testing and does not include optimization
    techniques such as KV caching or support for GPU-based inference and training. It was created primarily for personal learning purposes, so I encourage developers to
    use more reliable and well-established implementations.
    <br />
    First, download the necessary files (<code>config.json</code>, <code>model.safetensors</code>, and <code>tokenizer.json</code>) to run
    <a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct">Llama-3.2-1B-Instruct from the Hugging Face Hub</a>. Next, use these files along with the Hugging
    Face Transformers library to generate test cases in Python for verifying the correctness of the Rust implementation. In <code>retrieve_llama.py</code>, the output
    tensor from the first Decoder Block, along with those from its internal MLP Layer and Attention Layer, is examined. Additionally, the model's output token sequence
    for the input sentence "Who was the first president of the United States?" is observed.

    <br />
    <a href="retrieve_llama.py">[retrieve_llama.py]:</a>
    <pre data-src="retrieve_llama.py"></pre>

    Below is a portion of the results obtained by running on Python 3.12.6 with transformers==4.45.0, torch==2.5.1, safetensors==0.4.5, and tokenizers==0.20.3. It can be
    confirmed from the response to the question that the Llama 3.2 model possesses accurate historical knowledge.

    <pre><code class="language-bash">...
Class: &lt;class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'&gt;
Shape: [1, 10, 2048]
First: 0.489133
Min: 0.378667
Max: 0.635870
Mean: 0.500260
...
Tokens: [128000, 15546, 574, 279, 1176, 4872, 315, 279, 3723, 4273, 30, 10058, 6652, 198]
Text: '&lt;|begin_of_text|&gt;Who was the first president of the United States? George Washington\n'</code></pre>

    Here are the key components of the Llama implementation in Rust. Tests are implemented in llama_mlp.rs, llama_attention.rs, and llama_decoder_layer.rs, where it is
    verified that the outputs match those of Python. Additionally, in the tests for llama_model.rs, inference for the entire Llama model is conducted, and it is confirmed
    that the outputs align with Python's results.

    <pre><code class="language-toml">[dependencies]
candle-core = "0.8.0"
candle-nn = "0.8.0"
safetensors = "0.4.5"
tokenizers = "0.20.3"
half = "2.4.1"</code></pre>

    <br />
    <a href="config.rs">[config.rs]:</a>
    <pre data-src="config.rs" class="language-rust"></pre>

    <br />
    <a href="util.rs">[util.rs]:</a>
    <pre data-src="util.rs" class="language-rust"></pre>

    <br />
    <a href="llama_mlp.rs">[llama_mlp.rs]:</a>
    <pre data-src="llama_mlp.rs" class="language-rust"></pre>

    <br />
    <a href="llama_attention.rs">[llama_attention.rs]:</a>
    <pre data-src="llama_attention.rs" class="language-rust"></pre>

    <br />
    <a href="llama_decoder_layer.rs">[llama_decoder_layer.rs]:</a>
    <pre data-src="llama_decoder_layer.rs" class="language-rust"></pre>

    <br />
    <a href="llama_model.rs">[llama_model.rs]:</a>
    <pre data-src="llama_model.rs" class="language-rust"></pre>

    <hr />
  </body>
</html>
