
<ol>
  <h1>Compression & Acceleration</h1>
  
  <li>Xception: Deep Learning with Depthwise Separable Convolutions</li>
  <li>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model</li>
  <li>ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</li>
  <li>Model Compression</li>
  <li>Distilling the Knowledge in a Neural Network</li>
  <li>Do Deep Nets Really Need to be Deep?</li>
  <li>Face Model Compression by Distilling Knowledge from Neurons</li>
  <li>Reducing the Model Order of Deep Neural Networks Using Information Theory</li>
  <li>Optimal Brain Damage</li>
  <li>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</li>
  
  <li>BinaryConnect: Training Deep Neural Networks with binary weights during propagations</li>
  <li>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</li>
  <li>XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</li>
  <li>An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections</li>
  <li>Predicting Parameters in Deep Learning</li>
  <li>Restructuring of Deep Neural Network Acoustic Models with Singular Value Decomposition</li>
  <li>Compressing Neural Networks with the Hashing Trick</li>
  <li>Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications</li>
  <li>Speeding up Convolutional Neural Networks with Low Rank Expansions</li>
  <li>Learning Separable Filters</li>
  
  
  <li>Sparsifying Neural Network Connections for Face Recognition</li>
  <li>Optimal Brain Surgeon and general network pruning</li>
  <li>Structured Transforms for Small-Footprint Deep Learning</li>
  <li>Deep Fried Convnets</li>
  <li>Fast Algorithms for Convolutional Neural Networks</li>
  <li>cuDNN: Efficient Primitives for Deep Learning</li>
  <li>MEC: Memory-efficient Convolution for Deep Neural Network</li>
  <li>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</li>
  <li>Flattened Convolutional Neural Networks for Feedforward Acceleration</li>
  <li>Accelerating Very Deep Convolutional Networks for Classification and Detection (Efficient and Accurate Approximations of Nonlinear Convolutional Networks)</li>
  
  
  <li>Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition</li>
  <li>Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation</li>
  <li>Convolutional Neural Networks at Constrained Time Cost</li>
  <li>Quantized Convolutional Neural Networks for Mobile Devices</li>
  <li>Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial “Bottleneck” Structure</li>
  <li>Training CNNs with Low-Rank Filters for Efficient Image Classification</li>
  <li>Convolutional neural networks with low-rank regularization</li>
  <li>FitNets: Hints for Thin Deep Nets</li>
  <li>Fast ConvNets Using Group-wise Brain Damage</li>
  <li>LCNN: Lookup-based Convolutional Neural Network</li>
  
  
  <li>Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions</li>
  <li>ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections</li>
  <li>SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks</li>
  
  
  <h1>Unsupervised Image to Image Translation</h1>
  
  <li>Coupled Generative Adversarial Networks</li>
  <li>Unsupervised Image-to-Image Translation Networks</li>
  <li>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</li>
  <li>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</li>
  <li>StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</li>
 
  <h1>Interpretation</h1>
  
  <li>Understanding Deep Learning Generalization by Maximum Entropy</li>
  <li>Distilling a Neural Network Into a Soft Decision Tree</li>
  
  <h1>Other Machine Learning</h1>
  <li>Soft Decision Trees</li>
  
  <h1>Books</h1>
  <li>東京大学工学教程 基礎系 数学 微積分 東京大学工学教程編纂委員会編 時弘哲治著</li>
  
</ol>
